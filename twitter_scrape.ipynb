{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TWITTER SCRAPER\n",
    "\n",
    "A script to scrape for Twitter data using the Python package requests to retrieve the content and Beautifullsoup4 to parse the retrieved content. \n",
    "Search queries constructed with using Twitters advanced search: https://twitter.com/search-advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required Library\n",
    "\n",
    "import requests\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from const import USER_AGENT_LIST\n",
    "\n",
    "HEADER = {'User-Agent': random.choice(USER_AGENT_LIST)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://twitter.com/search?f=tweets&vertical=default&q={q}&max_position={pos}&l={lang}\"\n",
    "QUERY = \"{} since%3A{} until%3A{}&src=typd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_detail(tweet):\n",
    "    result = {        \n",
    "        'user_id' : tweet.find('span', 'username').text or '',\n",
    "        'user_name' : tweet.find('strong', 'fullname').text or '',\n",
    "        'text' : tweet.find('p', 'TweetTextSize').text or '',\n",
    "        'replies': tweet.find(\n",
    "            'span', 'ProfileTweet-action--reply u-hiddenVisually').find(\n",
    "            'span', 'ProfileTweet-actionCount')['data-tweet-stat-count'] or '0',\n",
    "        'retweet' : tweet.find(\n",
    "            'span', 'ProfileTweet-action--retweet u-hiddenVisually').find(\n",
    "            'span', 'ProfileTweet-actionCount')['data-tweet-stat-count'] or '0',\n",
    "        'like' : tweet.find(\n",
    "            'span', 'ProfileTweet-action--favorite u-hiddenVisually').find(\n",
    "            'span', 'ProfileTweet-actionCount')['data-tweet-stat-count'] or '0',\n",
    "        'timestamp' : dt.datetime.utcfromtimestamp(int(tweet.find(\n",
    "            'span', '_timestamp')['data-time'])),\n",
    "        'tweet_id' : tweet['data-item-id'] or \"\",\n",
    "        'url' : tweet.find('div', 'tweet')['data-permalink-path'] or \"\"\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(html):\n",
    "    soup = BeautifulSoup(html)\n",
    "    tweets = soup.find_all('li','js-stream-item')\n",
    "    if tweets:\n",
    "        for tweet in tweets:\n",
    "            try:\n",
    "                yield get_tweet_detail(tweet)\n",
    "            except AttributeError:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_single_page(url, retry=5):\n",
    "    \"\"\"\n",
    "    Returns tweets from the given URL.\n",
    "    :param url: The URL to get the tweets from\n",
    "    :param retry: Number of retries if something goes wrong.\n",
    "    :return: The list of tweets, the pos argument for getting the next page.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADER)\n",
    "        html = response.text or ''\n",
    "        tweets =list(get_tweets(html))\n",
    "        if not tweets:\n",
    "            if retry > 0:\n",
    "                logging.info('No new tweet, retrying... (Attempts left: {}) \\n{}'.format(retry, url))\n",
    "                return query_single_page(url, retry-1)\n",
    "            return [], None\n",
    "        return tweets, \"TWEET-{}-{}\".format(tweets[-1]['tweet_id'], tweets[0]['tweet_id'])\n",
    "    \n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        logger.exception('HTTPError {} while requesting \"{}\"'.format(\n",
    "            e, url))\n",
    "    except requests.exceptions.ConnectionError as e:\n",
    "        logger.exception('ConnectionError {} while requesting \"{}\"'.format(\n",
    "            e, url))\n",
    "    except requests.exceptions.Timeout as e:\n",
    "        logger.exception('TimeOut {} while requesting \"{}\"'.format(\n",
    "            e, url))\n",
    "    \n",
    "    if retry > 0:\n",
    "        logger.info('Retrying... (Attempts left: {})'.format(retry))\n",
    "        return query_single_page(url, retry-1)\n",
    "    \n",
    "    logger.error('STOP LOADING.')\n",
    "    return [], None\n",
    "\n",
    "#a,b = query_single_page(\"https://twitter.com/search?q=\"自宅\"%20AND%20\"仕事\"%20since%3A2006-03-21%20until%3A2018-08-17&l=ja&f=tweets&vertical=default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_all_once(query, limit = None, lang = None):\n",
    "    raw_query = query\n",
    "    logger.info(\"Querying {}. Limit number of tweet: {}\".format(query, limit))\n",
    "    query = query.replace(' ','%20').replace('#','%23').replace(':','%3A')\n",
    "    start = time.time()\n",
    "    pos = None\n",
    "    tweets = []\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            new_tweet, pos = query_single_page(URL.format(q=query,pos=pos,lang=lang))\n",
    "            logger.info(\"{} - {} new tweet. Total {}\".format(raw_query, len(new_tweet),len(tweets)))\n",
    "            if len(new_tweet) == 0:\n",
    "                logger.info(\"{} - Total: {}\".format(raw_query,len(tweets)))\n",
    "                return tweets\n",
    "            \n",
    "            tweets.extend(new_tweet)\n",
    "            \n",
    "            if limit and len(tweets) >= limit:\n",
    "                logger.info(\"{} - Total: {}\".format(raw_query,len(tweets)))\n",
    "                return tweets\n",
    "                             \n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"{} - Program interrupted. Returning tweets gathered \"\n",
    "                     \"so far...\".formate(raw_query))\n",
    "    end = time.time() \n",
    "    last_time = (end-start)/3600\n",
    "    #logger.info(\"{0} - Total:: {1} \\nScrape Time: {2:.3g} hour\".format(raw_query,len(tweets),last_time))\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def wrapper(args):\n",
    "    #return query_all_once(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pool(key_word, since=dt.date(2006,3,21), until=dt.date.today(), limit=None, lang=None, poolsize=20):\n",
    "    \n",
    "    start = time.time()\n",
    "    no_of_days = (until-since).days\n",
    "    if poolsize > no_of_days:\n",
    "        poolsize = no_of_days\n",
    "    \n",
    "    if limit:\n",
    "        limit_per_pool = (limit // poolsize) + 1\n",
    "    else:\n",
    "        limit_per_pool = None\n",
    "        \n",
    "    date_ranges = [since + dt.timedelta(days=elem) for elem in np.linspace(0, no_of_days, poolsize+1)]\n",
    "    date_blocks = zip(date_ranges[:-1], date_ranges[1:])\n",
    "    queries = [\"{} since:{} until:{}\".format(key_word,since,until) for since,until in date_blocks]\n",
    "    #para_tuples = [(query,limit_per_pool,lang) for query in queries]\n",
    "    all_tweets = []\n",
    "    try:\n",
    "        pool = Pool(poolsize)\n",
    "        try:\n",
    "            for new_tweets in pool.imap_unordered(partial(query_all_once, limit=limit_per_pool, lang=lang), queries):\n",
    "                    all_tweets.extend(new_tweets)\n",
    "                    logger.info(\"----NEW TWEETS: {} ALL TWEETS: {}----\".format(len(new_tweets),len(all_tweets)))\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            logger.info('Program interrupted by user. Returning all tweets gathered so far.')\n",
    "    finally:\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        \n",
    "    end = time.time() \n",
    "    last_time = (end-start)/3600\n",
    "    logger.info(\"Scrape Time: {0:.3g} hour\".format(last_time))\n",
    "    return all_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(tweets, filename='test_output.csv'):\n",
    "    df = pd.DataFrame(tweets)\n",
    "    df.to_csv(filename, sep=',', encoding = 'utf-8', index = False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_word = '遊園地 since:2012-01-01 until:2013-01-01'\n",
    "results = query_all_once(query=key_word,lang = 'ja',limit=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)\n",
    "#to_csv(results, filename='dataset/自宅AND警備_{}.csv'.format(dt.date.today()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage:\n",
    " - Function search_twitter(key_word, since, until, filename, limit, lang)\n",
    " - Input Parameter:\n",
    "     - key_word: specific keyword you want to search in twitter\n",
    "     - since: begin date, default 2006-03-21. Format yyyy-MM-dd\n",
    "     - until: end date, default today. Format yyyy-MM-dd\n",
    "     - filename: name of csv output file. If not provided, output will save to twitter_output.csv by default\n",
    "     - limit: number of tweet data you want to get. If not provided, will get as much as possible\n",
    "     - lang: Set this if you want to query tweets in a specific language. (For example Japanese is ja)\n",
    "     \n",
    "     \n",
    " - The script scrapes the following information: \n",
    "     + Username and Full Name \n",
    "     + Tweet-id \n",
    "     + Tweet text \n",
    "     + Tweet timestamp \n",
    "     + No. of likes \n",
    "     + No. of replies \n",
    "     + No. of retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "fh = logging.FileHandler('log/scraping.log.{}'.format(dt.date.today()))\n",
    "fh.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "logger.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "# create file handler which logs even debug messages\n",
    "fh = logging.FileHandler('log/scraping.log.{}'.format(dt.date.today()))\n",
    "#fh.setLevel(logging.DEBUG)\n",
    "# create console handler with a higher log level\n",
    "#ch = logging.StreamHandler()\n",
    "#ch.setLevel(logging.ERROR)\n",
    "# create formatter and add it to the handlers\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "#ch.setFormatter(formatter)\n",
    "fh.setFormatter(formatter)\n",
    "# add the handlers to logger\n",
    "#logger.addHandler(ch)\n",
    "logger.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAMPLE SCRAPPING FOR HASHTAG WORLDCUP\n",
    "\n",
    "key_word = '\"自宅\" AND \"仕事\"'\n",
    "#since = '2018-06-15'\n",
    "#until = '2018-07-16'\n",
    "filename = 'test.csv'\n",
    "lang = \"ja\"\n",
    "limit = None\n",
    "\n",
    "search_twitter(key_word=key_word, filename=filename, limit=limit, lang=lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start time: 2018-08-15 05:15:37,328\n",
    "End Time: 2018-08-15 14:12:26,523"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_oj = dt.datetime.strptime(\"\",%Y-%m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393180\n",
      "230723\n",
      "323016\n"
     ]
    }
   ],
   "source": [
    "#Read csv and analyse\n",
    "files = [\"twitter_scrape_自宅AND仕事.csv\",\"twitter_scrape_自宅AND勉強.csv\",\"twitter_scrape_自宅AND警備.csv\"]\n",
    "for f in files:\n",
    "    df = pd.read_csv(f)\n",
    "    print len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset/自宅AND警備_2018-08-21.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "361121"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df[\"timestamp\"].apply(lambda x: dt.datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2007      301\n",
       "2008      973\n",
       "2009     9612\n",
       "2010    25011\n",
       "2011    50037\n",
       "2012    36034\n",
       "2013    39007\n",
       "2014    50036\n",
       "2015    25020\n",
       "2016    50037\n",
       "2017    45571\n",
       "2018    29482\n",
       "Name: year, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['year'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018    221805\n",
       "2017    101211\n",
       "Name: year, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"twitter_scrape_自宅AND警備.csv\")\n",
    "df['year'] = df[\"timestamp\"].apply(lambda x: dt.datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").year)\n",
    "df['year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'自宅' and  in df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in df['text']:\n",
    "    if not '自宅' and '警備' in t:\n",
    "        print t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
